---
title: "cleaning_n_descriptive"
output: pdf_document
date: "2023-04-26"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### A. Load all relevant packages and set directories

```{r, message=FALSE, warning = FALSE}
require(quanteda)
require(quanteda.textmodels)
require(quanteda.textstats)
require(quanteda.textplots)
require(quanteda.corpora)

library(ggrepel)
library(readxl)
library(rvest)
require(readtext)
require(devtools)
require(tidyverse)
require(ggplot2) 
library(stringr)
library(purrr)
library(readr)
library(lubridate)
library(scales)
```

```{r, message=FALSE, warning = FALSE}
rm(list = ls())
DATA_PATH = "/Users/hugosalas/Roemer & Salas/data"
UTILS_PATH = "/Users/hugosalas/Roemer & Salas/"

source(str_c(UTILS_PATH, 'data_manipulation_utils.R'))
```

### B. Load all relevant files and DF

```{r, message=FALSE}

## Article data from our 5 countries
FILES_TO_LOAD = c("all_articles_CANADA_df", "all_articles_JAPAN_df", 
                  "all_articles_GERMANY_df", "all_articles_CHINA_df", 
                  "all_articles_MEXICO_df", 'all_articles_presidents_df',
                  "all_articles_years_df")

# Store all dfs in a list
trade_partners_lst = list()

for (file in FILES_TO_LOAD){
  load(str_c(DATA_PATH,'/' , file))
  
  if (file != 'all_articles_presidents_df' &
      file != 'all_articles_years_df'){
    country_name = str_replace(file, "all_articles_", "")
    country_name = str_replace(country_name, "_df", "")
    
    if (file != "all_articles_MEXICO_df"){
      trade_partners_lst[[country_name]] = country_df
      rm(country_df)
    } else {
      trade_partners_lst[[country_name]] = mexico_df
      rm(mexico_df)
    }
  }
}

## Data at the president level with party membership, legnth of stay, among others
party_df <- read_csv(str_c(DATA_PATH, "/presidents.csv"))
```

### C. Data cleaning and manipulation

```{r, message=FALSE}
## Clean the data we have on presidents
party_df = clean_party_df(party_df)

## Cleaning our doc data from all our countries
for (country in seq_along(trade_partners_lst)){
  # Reformat dates in all DFs so that they look intuitive
  trade_partners_lst[[ country ]]$date = as.Date(as.numeric(trade_partners_lst[[ country ]]$date))
  # Add last name and year to DF
  trade_partners_lst[[ country ]] = add_last_name_n_year_to_df( trade_partners_lst[[ country ]] )
  # Merge with party/president data
  trade_partners_lst[[ country ]] <- trade_partners_lst[[ country ]] %>% 
    left_join(party_df, by = c("last_name" = "last_name"))
  # Recategorize party variable so it separates between democrats and republicans
  trade_partners_lst[[ country ]] <- recategorize_parties(trade_partners_lst[[ country ]])
}
```

### D. Quick data exploration to get some truths about each country

```{r, message=FALSE}
for (country in names(trade_partners_lst)){
  print(str_c('############## ', country, ' ############'))
  
  nrows = nrow( trade_partners_lst[[ country ]] )
  print(str_c('Number of rows/docs: ', nrows ))
  
  print('Party Membership %')
  print(table( trade_partners_lst[[ country ]]$Party)/nrows*100)
  
  print('Doc category % (top 7)')
  print(sort(colSums( trade_partners_lst[[ country ]][,5:74], na.rm = TRUE), decreasing = TRUE)[1:7]/nrows)
  
  print(str_c('Share of docs not coming directly from the president: ', sum(is.na(trade_partners_lst[[ country ]]$Party))/nrows*100))
  
  for (i in 1:3){
    cat("\n")
  }
}
```

### E. Mentions by year per country

```{r, warning=FALSE}
counts_year = data.frame()

for (country in seq_along(trade_partners_lst)){
  
  # Group by year while we count
  groupby_temp <- trade_partners_lst[[country]]%>% 
    group_by(year) %>% 
     summarise(n_docs = n())
  
  # Add a column to identify each batch/country
  groupby_temp['country'] = names(trade_partners_lst)[country]
  
  # Append to df
  counts_year = rbind(counts_year, groupby_temp)
}

counts_year = counts_year[counts_year$year!=2023,]

# Since there wasn't much happening before 1900, let's create several graphs 
#   with varying start years
for (start_year in c(1800,1900, 1950, 1980)){
  plot = ggplot(counts_year[counts_year['year']>start_year,], aes(year, n_docs, color = country)) + 
    geom_line(aes(group=country)) +
    theme_test()+
    scale_x_continuous(breaks = pretty_breaks(n = 10))+
    scale_y_continuous(breaks = pretty_breaks(n = 5))+
    theme(axis.text.x = element_text(angle = 0, vjust = .7))+
    labs(title = '" ',
         subtitle = "",
         y = "", 
         x = "", 
         caption = "",
         color = 'Legend')
  
  print(plot)
}
```

```{r, warning=FALSE}
counts_since2010 <- counts_year[counts_year$year>2010,] %>%
  group_by(country) %>%
  summarize(average = mean(n_docs))

ggplot(data = counts_since2010, aes(x = reorder(country, average), y = average, fill=country)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  geom_text(data = counts_since2010, aes(x = country, y = average, label = average), vjust = -0.5, color = "black") +
  labs(title = 'Promedio de menciones desde 2010',
       subtitle = "",
       y = "", 
       x = "", 
       caption = "")
```

### F. Mexico mentions by year

#### i) Raw count

```{r, warning=FALSE}
Y_VAL = 200
COLOR = "blue"
SIZE = 3

counts_year[counts_year$country=='MEXICO',] %>% 
  ggplot(aes(x = year, y = n_docs)) +
  geom_line() +
  theme_test() +
  scale_x_continuous(breaks = pretty_breaks(n = 10)) +
  scale_y_continuous(breaks = pretty_breaks(n = 5)) +
  geom_vline(aes(xintercept = 1847.5), linetype = "dotted", color = COLOR)+   
  annotate("text", x=1827, y=Y_VAL, label="1848 - Fin de Guerra \nMéxico-Americana ", color = COLOR, size = SIZE)+
  geom_vline(aes(xintercept = 1864), linetype = "dotted", color = COLOR)+   
  annotate("text", x=1872, y=Y_VAL, label="1862 - \n                         Maximiliano en México", color = COLOR, size = SIZE)+
  geom_vline(aes(xintercept = 1923), linetype = "dotted", color = COLOR) +   
  annotate("text", x=1937, y=Y_VAL, label="1923 - \nSuspensión \nde relaciones \ndiplomáticas", color = COLOR, size = SIZE) +
  geom_vline(aes(xintercept = 1993), linetype = "dotted", color = COLOR) +   
  annotate("text", x=1985, y=Y_VAL, label="1993 - \nTLCAN", color = COLOR, size = SIZE) +
  geom_vline(aes(xintercept = 2017), linetype = "dotted", color = COLOR) +   
  annotate("text", x=2010, y=Y_VAL, label="2017 - \nTrump", color = COLOR, size = SIZE)+
  labs(title = 'Documentos presidenciales con mención de "México" ',
       subtitle = "",
       y = "", 
       x = "", 
       caption = "*discursos disponibles en https://www.presidency.ucsb.edu/")
```

#### ii) Normalized by total docs by year

```{r, warning=FALSE}
counts_year = inner_join(all_articles_years_df, counts_year, by = "year")
Y_VAL = 25

counts_year['share'] = counts_year$n_docs/as.integer(counts_year$records)*100

counts_year[counts_year$country=='MEXICO',] %>% 
  ggplot(aes(x = year, y = share)) +
  geom_line() +
  theme_test() +
  scale_x_continuous(breaks = pretty_breaks(n = 10)) +
  scale_y_continuous(breaks = pretty_breaks(n = 5)) +
  geom_hline(yintercept = mean(counts_year[counts_year$country=='MEXICO','share']), color = 'red') +
  geom_vline(aes(xintercept = 1847.5), linetype = "dotted", color = COLOR)+   
  annotate("text", x=1827, y=Y_VAL, label="1848 - Fin de Guerra \nMéxico-Americana ", color = COLOR, size = SIZE)+
  geom_vline(aes(xintercept = 1864), linetype = "dotted", color = COLOR)+   
  annotate("text", x=1872, y=Y_VAL, label="1862 - \n                         Maximiliano en México", color = COLOR, size = SIZE)+
  geom_vline(aes(xintercept = 1923), linetype = "dotted", color = COLOR) +   
  annotate("text", x=1937, y=Y_VAL, label="1923 - \nSuspensión \nde relaciones \ndiplomáticas", color = COLOR, size = SIZE) +
  geom_vline(aes(xintercept = 1993), linetype = "dotted", color = COLOR) +   
  annotate("text", x=1985, y=Y_VAL, label="1993 - \nTLCAN", color = COLOR, size = SIZE) +
  geom_vline(aes(xintercept = 2017), linetype = "dotted", color = COLOR) +   
  annotate("text", x=2010, y=Y_VAL, label="2017 - \nTrump", color = COLOR, size = SIZE)+
  labs(title = 'Documentos presidenciales con mención de "México" ',
       subtitle = "Dividido por total de documentos ese ano",
       y = "", 
       x = "", 
       caption = "*discursos disponibles en https://www.presidency.ucsb.edu/")
```

### G. Mexico mentions by president

#### i) Raw count

```{r}
VICE_PRESIDENT_CATEGORIES <- c("Vice Presidential", "Vice Presidential Candidates", 
                               "Vice President's Press Secretary", 
                               "Vice Presidential Debates", "Vice President's Statements",
                               "Vice President's Remarks", "First Lady Press",
                               "First Lady")

temp_df = trade_partners_lst$MEXICO

#Remove vice presidents and first ladies
for (category in VICE_PRESIDENT_CATEGORIES){
  temp_df = temp_df[temp_df[category]==0,]
}

ndoc_pres <- temp_df %>% 
  group_by(last_name, speaker) %>% 
  summarise(n_docs = n()) %>% 
  arrange(desc(n_docs)) 

# Pegar datos adicionales de presidente y partido
ndoc_pres <- ndoc_pres %>% 
  right_join(party_df, by = c("last_name" = "last_name")) #Cleveland has two nonconsecutive terms

# Traducir el nombre de partidos a español
ndoc_pres <- ndoc_pres %>% 
  mutate(Partido = if_else(str_detect(Party,"Democratic"), "Democrático",
                           if_else(str_detect(Party, "Republican"), "Republicano", Party))) 

#Eliminar Cleveland que esta repetido
ndoc_pres = ndoc_pres[ndoc_pres$last_name!="Cleveland (second term)",]

#Nos quedamos con top 15
top15_df <- ndoc_pres %>% 
  arrange(desc(n_docs)) %>% 
  head(15)
  
top15_df %>% 
  ggplot(aes(x = fct_reorder(last_name, n_docs), y = n_docs, fill = Partido))+
  geom_col()+
  geom_text(aes(label = n_docs), vjust = 0.5) +
  theme_bw()+
  scale_y_continuous(breaks = pretty_breaks(n = 10))+
  scale_fill_manual(values = wesanderson::wes_palette("Moonrise3", n = 3))+
  labs(title = 'Número de documentos presidenciales con término "México"', 
       y = "Número de Documentos", 
       x = "") +
  coord_flip()

```

#### ii) Normalized by total docs by president

```{r}
ndoc_pres = inner_join(all_articles_presidents_df, ndoc_pres, by = c("president" = "speaker"))
ndoc_pres['share'] = ndoc_pres$n_docs/as.integer(ndoc_pres$records)*100

#Nos quedamos con top 15
top15_df <- ndoc_pres %>% 
  arrange(desc(share)) %>% 
  head(15)
  
top15_df %>% 
  ggplot(aes(x = fct_reorder(last_name, share), y = share, fill = Partido))+
  geom_col()+
  geom_text(aes(label = share), vjust = 0.5) +
  theme_bw()+
  scale_y_continuous(breaks = pretty_breaks(n = 10))+
  scale_fill_manual(values = wesanderson::wes_palette("Moonrise3", n = 3))+
  labs(title = 'Número de documentos presidenciales con término "México"', 
       y = "% del total de docs", 
       x = "") +
  coord_flip()
```

## Converting dataframe to corpus

```{r,warning=FALSE}
corpus_usmex <- corpus(usmex_df, text_field = "text")
save(corpus_usmex,file = "data/corpus_usmex")
#summary(corpus_usmex)
class(corpus_usmex)
#as.character(corpus_usmex)[300]
```

## Subsetting

I decide to create different corpora, in case I may need them further down.

```{r}
post_1900 <- corpus_usmex %>% 
  corpus_subset(year >= 1900)
save(post_1900,file = "data/corpus_post1900")


Trump_Biden <- corpus_usmex %>% 
  corpus_subset(last_name %in% c("Trump", "Biden"))
save(Trump_Biden,file = "data/corpus_TrumpBiden")

republicans <- corpus_usmex %>% 
  corpus_subset(Party == "Republican")
save(republicans,file = "data/corpus_republicans")

democratic <- corpus_usmex %>% 
  corpus_subset(Party == "Democratic")
save(democratic,file = "data/corpus_democratic")
```

## Preprocessing

-   Tokenizing (bag of word assumption)
-   Discarding stop words
-   Removing capitalization, punctuation and numbers
-   Combining similar terms (lemmatizing)

```{r}
toks_usmex<- corpus_usmex %>%
  tokens(split_hyphens = T,
         remove_punct = TRUE,
         remove_numbers = TRUE,
         remove_symbols = TRUE) %>%
  tokens_tolower(keep_acronyms = T) %>% # I decided to keep acronyms (govt doc!)
  tokens_remove(pattern = stopwords("en")) %>% 
  tokens_wordstem()

#head(toks_usmex)
```

Let's look at how our tokenized corpus would look with different n-grams.

```{r}
toks_ngram <- corp_drug_speeches %>%
  tokens(split_hyphens = T,
         remove_punct = TRUE,
         remove_numbers = TRUE,
         remove_symbols = TRUE) %>%
  tokens_tolower(keep_acronyms = T) %>% # I decided to keep acronyms (govt doc!)
  tokens_remove(pattern = stopwords("en")) %>% 
  tokens_wordstem() %>% 
  tokens_ngrams(n = 2:4)

#head(toks_ngram)
```

## Creating Document Feature Matrix (DTM)

```{r}
dtm_drugs <- dfm(toks_drugs)
dtm_drugs_ngram <- dfm(toks_ngram)
```

### Exploring our DTM

```{r}
ndoc(dtm_drugs)
nfeat(dtm_drugs)

#document length 
by_docs <- rowSums(dtm_drugs)

#top features
topfeatures(dtm_drugs, n = 200)

dtm_drugs_weight <- dfm_weight(dtm_drugs, scheme = "prop")
#dtm_drugs_weight
```

### save corpora

```{r}
save(toks_drugs, file = "data/toks_drugs")
save(toks_ngram, file = "data/toks_ngram")
save(dtm_drugs_ngram, file = "data/dtm_drugs_ngram")
save(dtm_drugs, file = "data/dtm_drugs")
save(drugs_df, file = "data/drug_speeches")
save(party, file = "data/party")
save(total_speeches, file = "data/total_speeches")
```
