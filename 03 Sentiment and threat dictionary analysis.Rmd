---
title: "sentiment_n_threat_dictionary"
author: "Alejandro Roemer & Hugo Salas"
date: "2023-06-25"
output: pdf_document
---

This was originally 'dictionary_project.rmd'.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### A. Load all relevant packages and set directories

```{r, message=FALSE}
library(tidyverse)
library(textdata)
library(quanteda)
#require(quanteda.corpora)
library(tidytext)
library(scales)
# Check if devtools package is installed
if (!requireNamespace("devtools", quietly = TRUE)) {
  # Install devtools package
  install.packages("devtools")
  
  # Load devtools package
  library(devtools)
} else {
  # Load devtools package
  library(devtools)
}
```

```{r}
rm(list = ls())

if (Sys.info()["user"]=='hugosalas'){
  BASE_PATH = "/Users/hugosalas/Roemer & Salas/"
} else if (Sys.info()["user"]=='aleroemer'){
  BASE_PATH = "~/My Drive/DiscursoUSA_MX/"
}

DATA_PATH =  paste0(BASE_PATH, "data/")
CLEAN_DATA_PATH = paste0(DATA_PATH, "clean/")
UTILS_PATH = paste0(BASE_PATH, "USMEX_TextAnalysis/utils/")

source(str_c(UTILS_PATH, 'data_manipulation_utils.R'))
```

### B. Load all relevant files and DF

```{r}
load(paste0(CLEAN_DATA_PATH, "clean_articles_ALL"))

threat_dict <- read.delim(paste0(DATA_PATH, "threat.txt"),
                          header = FALSE) %>% 
  rename(threat_features = V1)

extra_words <- tibble(
  threat_features = c("addiction","overdose", "poison", "poisoning",
                      "gang", "gangs", "cartel", "addict", "addicts", "dealer"))

threat_dict <- threat_dict %>% 
  rbind(extra_words)
```

### C. Threat dictionary analysis (Choi et al. (2022))

This dictionary does not have a sentiment associated to it, therefore we cannot use the quanteda function as.dictionary(). Instead, following the methodology by I will only be able to count the number of matches and the percent of threat

#### i. MEXICO: Figure out which words in our paragraphs are 'threat' words

```{r}
current_country = 'MEXICO'

#Create unique text ID column
trade_partners_lst[[current_country]] = create_unique_id(current_country)
country_df = trade_partners_lst[[current_country]]
save(country_df, file = paste0(CLEAN_DATA_PATH, "clean_articles_", current_country))

#Create a paragraph df with only paragraphs that mention MEXICO
paragraphs_df = separate_text_into_paragraphs(country = current_country)
save(paragraphs_df, file = paste0(CLEAN_DATA_PATH, "paragraphs_", current_country))

tokens_df <- unnest_tokens(paragraphs_df, word_tokens, text_paragraph, token = "words")

# Eliminate stopwords
data(stop_words)
tokens_df_nosw <- tokens_df %>%
  anti_join(stop_words, by = c("word_tokens" = "word"))

# Identify threat tokens
tokens_df <- tokens_df_nosw %>% 
  mutate(threat_feature = if_else(word_tokens %in% threat_dict$threat_features, 1, 0))

rm(tokens_df_nosw)
# Count all tokens
tokens_df$count = 1
```

#### ii. MEXICO: Most used 'threat' words in all documents related to Mexico

```{r}
tokens_df %>% 
  filter(threat_feature == 1) %>% 
  mutate(word_tokens = if_else(word_tokens == "problem" | word_tokens == "problems", "problem", word_tokens),
         word_tokens = if_else(word_tokens == "concern" | word_tokens == "concerns", "concern", word_tokens)) %>% 
  count(word_tokens) %>% 
  arrange(desc(n)) %>%
  head(20) %>% 
  ggplot(aes(x = fct_reorder(word_tokens,n), n))+
  geom_col(fill = "lightblue")+
  theme_bw()+
  coord_flip() +
  theme(axis.text.x = element_text(angle = 45, vjust = .7))+
  scale_y_continuous(breaks = seq(0,2000,100))+
  labs(title = 'Palabras más comunes del "threat dictionary"',
       x = "", y = "Frecuencia", 
       caption = '*Consultar https://www.pnas.org/doi/10.1073/pnas.2113891119') 


# What if we filter post 1950? 
tokens_df %>% 
  filter(threat_feature == 1, year >= 1950) %>% 
  mutate(word_tokens = if_else(word_tokens == "problem" | word_tokens == "problems", "problem", word_tokens),
         word_tokens = if_else(word_tokens == "concern" | word_tokens == "concerns", "concern", word_tokens)) %>% 
  count(word_tokens) %>% 
  arrange(desc(n)) %>%
  head(20) %>% 
  ggplot(aes(x = fct_reorder(word_tokens,n), n))+
  geom_col(fill = "lightblue")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 45, vjust = .7))+
  scale_y_continuous(breaks = seq(0,2000,100))+
  coord_flip() +
  labs(title = 'Palabras más comunes del "threat dictionary""',
       x = "", y = "Frecuencia", 
       caption = '*Consultar https://www.pnas.org/doi/10.1073/pnas.2113891119') 


```

#### iii. MEXICO: Threat words over time

```{r}
# Aggregate by text
agg_threats <- tokens_df %>%
  group_by(text_ID) %>%
  summarise(n_threat_tokens = sum(threat_feature),
            n_tokens = sum(count))

trade_partners_lst[[current_country]] <- trade_partners_lst[[current_country]] %>%
  left_join(agg_threats, by = c("text_ID" = "text_ID")) %>%
  mutate(n_threat_tokens = if_else(is.na(n_threat_tokens), 0, n_threat_tokens),
         n_tokens = if_else(is.na(n_tokens), 0, n_tokens))

trade_partners_lst[[current_country]]$count = 1

# Group by year
groupby_temp <- trade_partners_lst[[current_country]]%>% 
  group_by(year) %>% 
  summarise(total_threat_tokens = sum(n_threat_tokens),
            total_tokens = sum(n_tokens),
            total_docs = sum(count) )

groupby_temp$threat_per_token = groupby_temp$total_threat_tokens/groupby_temp$total_tokens
groupby_temp$threat_per_doc = groupby_temp$total_threat_tokens/groupby_temp$total_docs
  
```

```{r}
groupby_temp %>% 
  ggplot(aes(x = year, y = threat_per_token)) +
  geom_line() +
  theme_test() +
  scale_x_continuous(breaks = pretty_breaks(n = 10)) +
  scale_y_continuous(breaks = pretty_breaks(n = 5))


groupby_temp %>% 
  arrange(desc(threat_per_token)) %>% #algo raro: revisar año 1846
  head(10)


plot = groupby_temp %>% 
  ggplot(aes(x = year, y = threat_per_token)) #revisar año 1846

  add_annotations_to_mx_time_series_plot(plot, 
                                       text_color = 'purple', 
                                       plot_title = '# of threat words per token',
                                       text_y_val = 0.03,
                                       bottom_note_text = '*discursos disponibles en https://www.presidency.ucsb.edu/')

  
#### hay un problema con la serie de tiempo!!!
#tarea ale: explorar top picos... probablemente tengamos que cambiar los labels. 
#Analyzing top threat_per_token years: 
groupby_temp %>% 
  arrange(desc(threat_per_token)) %>% #algo raro: revisar año 1846
  head(10)

```

```{r}
plot = groupby_temp %>% 
  ggplot(aes(x = year, y = threat_per_doc))

add_annotations_to_mx_time_series_plot(plot, 
                                       text_color = 'purple', 
                                       plot_title = '# of threat words per doc',
                                       bottom_note_text = '*discursos disponibles en https://www.presidency.ucsb.edu/')
```

#### iv. MEXICO: Threat words by Party
```{r}
#changed formula of split_into_paragraphs AR
##### Contando total de tokens por partido

## Crudo
 tokens_df %>% 
  filter(threat_feature == 1) %>%
  mutate(word_tokens = if_else(word_tokens == "problem" | word_tokens == "problems", "problem", word_tokens),
         word_tokens = if_else(word_tokens == "concern" | word_tokens == "concerns", "concern", word_tokens)) %>% 
  group_by(Party, word_tokens) %>% 
  summarise(n = n())%>% 
  ungroup() %>% 
  arrange(desc(n)) %>%
  head(25)   %>% 
   ggplot(aes(x = fct_reorder(word_tokens,n), y = n, fill = Party))+ #should normalize by total party tokens
  geom_col(position = "dodge2") +
  theme_bw()+
  scale_fill_manual(values = wesanderson::wes_palette("Moonrise3", n = 2))+
  theme(axis.text.x = element_text(angle = 45, vjust = .7))+
  labs(title = 'Most common features from "threat dictionary"',
       x = "", y = "Frequency", 
       caption = '*Consult at https://www.pnas.org/doi/10.1073/pnas.2113891119')
 
 
### Ponderando por número total de tokens por partido 
tokens_df %>% 
  count(Party) 

total_toks_dem <- 205798
total_toks_rep <- 196421	 

threat_party_graphs(1950)

#post 1970
threat_party_graphs(1970)

#post 1990
threat_party_graphs(1990)

# post 2000
threat_party_graphs(2000) ### REPUBLICANOS SE CONVIERTEN MUCHO MAS DRASTICOS RELATIVO A DEMOCRATAS
```

#### v. Threat words por presidente

```{r, warning=FALSE}
 threat_pres <- tokens_df %>% 
  filter(threat_feature == 1) %>%
  mutate(word_tokens = if_else(word_tokens == "problem" | word_tokens == "problems", "problem", word_tokens),
         word_tokens = if_else(word_tokens == "concern" | word_tokens == "concerns", "concern", word_tokens)) %>% 
  group_by(last_name) %>% 
  summarise(n_threats = n(),
            Party = Party)%>% 
  slice(1) %>% 
  arrange(desc(n_threats)) 
  
 total_tokens_pres<- tokens_df %>% 
 count(last_name) %>% 
 rename(total_toks = n)
  
 threat_pres <- threat_pres %>% 
   left_join(total_tokens_pres, by = "last_name") %>% 
   mutate(n_weighted = round(n_threats/total_toks*100,2)) %>% 
   head(15)
 
 threat_pres %>% 
  ggplot(aes(x = fct_reorder(last_name,n_weighted), n_weighted, fill = Party))+
  geom_col()+
  theme_bw()+
  scale_fill_manual(values = wesanderson::wes_palette("Moonrise3", n = 3)) +
  labs(title = 'Palabras más comunes del "threat dictionary" por presidente',
       subtitle = 'Como porcentaje del total de palabras en párrafos que contienen el término "México"',
       x = "", y = "%", 
       caption = '*Consultar diccionario https://www.pnas.org/doi/10.1073/pnas.2113891119')+
    coord_flip()

```


### D. MEXICO: Sentiment analysis using Python's flair

```{python}
import pyreadr #'!pip3 install pyreadr'
from utils.python_utils import analyze_paragraphs_sentiment

CLEAN_DATA_PATH = '../data/clean/'

# Load all articles
articles_df = pyreadr.read_r(CLEAN_DATA_PATH + "clean_articles_MEXICO")['country_df']
paragraphs_df = pyreadr.read_r(CLEAN_DATA_PATH + 'paragraphs_MEXICO')['paragraphs_df']

# Add sentiment columns to our dataframe
paragraphs_df = analyze_paragraphs_sentiment(paragraphs_df)
paragraphs_df.to_csv(CLEAN_DATA_PATH + "paragraphs_sentiment_" + 'MEXICO')
```

```{python}
import pandas as pd
import matplotlib.pyplot as plt

# Add the date to the df
paragraphs_df = paragraphs_df.merge(articles_df[['date', 'text_ID']], on='text_ID', how='left')
paragraphs_df.date = pd.to_datetime(paragraphs_df['date'])
paragraphs_df['year'] = paragraphs_df['date'].dt.year
```


```{python}
# % of positive sentiments in 
for start_year in [1800,1900, 1950, 1980]:
  groupby_temp = paragraphs_df.groupby('year')['sentiment_label_is_positive'].mean()
  
  groupby_temp[groupby_temp.index>start_year].plot()
  plt.show()
```



```{python}
# raw number of positive sentiments in 
for start_year in [1800,1900, 1950, 1980]:
  groupby_temp = paragraphs_df.groupby('year')['sentiment_label_is_positive'].sum()
  
  groupby_temp[groupby_temp.index>start_year].plot()
  plt.show()
```

# V. key words in context (kwic)

## 1. document level

```{r}
#toks_kwic <- tokens(corp_drug_speeches, remove_punct = TRUE) %>% 
#  tokens_remove(pattern = stopwords("en"))
load("data/corp_drug_speeches")
toks_kwic <-corp_drug_speeches %>%
  tokens(split_hyphens = TRUE,
         remove_punct = TRUE,
         remove_numbers = TRUE,
         remove_symbols = TRUE) %>%
  tokens_tolower() %>% 
  tokens_remove(pattern = stopwords("en")) 

kwic_party <- toks_kwic %>% 
  tokens_group(groups = last_name)

textplot_xray(kwic(kwic_party, pattern = "drug*"))


#get relevant keywords and phrases
drug_words <- c("drug*")

toks_drugkwic <- tokens_keep(toks_kwic, pattern = phrase(drug_words), window = 5)
```

## 3. paragraph level

```{r}
corp_par <- corpus(drugs_df_paragraph2, text_field = "text_paragraph")
save(corp_par, file = "data/corpus_paragraph")

toks_kwic <-corp_par %>%
  tokens(split_hyphens = TRUE,
         remove_punct = TRUE,
         remove_numbers = TRUE,
         remove_symbols = TRUE) %>%
  tokens_tolower() %>% 
  tokens_remove(pattern = stopwords("en")) 

kwic_party <- toks_kwic %>% 
  tokens_group(groups = last_name)

quanteda.textplots::textplot_xray(kwic(kwic_party, pattern = "drug*"))

```

### Distinctiveness

What words make it easiest to distinguish between democrats and republicans?
